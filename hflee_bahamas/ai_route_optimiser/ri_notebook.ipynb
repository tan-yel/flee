{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElqeBhSo7_ML"
      },
      "source": [
        "# Q-Learning Prototype for Hurricane Evacuation\n",
        "\n",
        "This notebook demonstrates a simplified reinforcement learning prototype for learning optimal evacuation routes during a hurricane event. The environment consists of five locations with varying hurricane severity, and the agent learns to avoid danger zones while reaching a safe location.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qyZLOto8K9A"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.11.0' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/usr/local/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import networkx as nx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkjM4stL8TLR"
      },
      "outputs": [],
      "source": [
        "# Graph structure: adjacency list\n",
        "graph = {\n",
        "    0: [1, 2],   # 0 = Start\n",
        "    1: [3],\n",
        "    2: [3, 4],\n",
        "    3: [4],      # 4 = Safe Zone\n",
        "    4: []        # Terminal\n",
        "}\n",
        "\n",
        "location_names = {\n",
        "    0: \"marsh_harbor\",\n",
        "    1: \"road_a\",\n",
        "    2: \"road_b\",\n",
        "    3: \"junction\",\n",
        "    4: \"nassua (safe zone)\"\n",
        "}\n",
        "\n",
        "hurricane_levels = {\n",
        "    0: 4,\n",
        "    1: 2,\n",
        "    2: 3,\n",
        "    3: 1,\n",
        "    4: 0\n",
        "}\n",
        "\n",
        "hurricane_impact_map = {\n",
        "    1: -1,\n",
        "    2: -3,\n",
        "    3: -5,\n",
        "    4: -8,\n",
        "    5: -10\n",
        "}\n",
        "\n",
        "evacuation_threshold = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIsqgDDZ9sac"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "gamma = 0.8  # Discount factor\n",
        "alpha = 0.1  # Learning rate\n",
        "episodes = 500\n",
        "epsilon = 0.2  # Exploration factor\n",
        "\n",
        "# Q-table: [states x actions]\n",
        "q_table = np.zeros((len(graph), len(graph)))\n",
        "\n",
        "def get_reward(current, next):\n",
        "    if next == 4:\n",
        "        return 10\n",
        "    elif hurricane_levels[next] >= evacuation_threshold:\n",
        "        return hurricane_impact_map[hurricane_levels[next]]\n",
        "    else:\n",
        "        return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zd5CcSoi9zey"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "for ep in range(episodes):\n",
        "    state = 0\n",
        "    while state != 4:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = random.choice(graph[state])\n",
        "        else:\n",
        "            q_vals = [q_table[state][a] for a in graph[state]]\n",
        "            best_action_idx = np.argmax(q_vals)\n",
        "            action = graph[state][best_action_idx]\n",
        "\n",
        "        reward = get_reward(state, action)\n",
        "        future = max([q_table[action][a] for a in graph[action]] or [0])\n",
        "        q_table[state][action] += alpha * (reward + gamma * future - q_table[state][action])\n",
        "\n",
        "        state = action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-0SNRZy94mp",
        "outputId": "74949748-9459-4170-f51c-0b3796d2d129"
      },
      "outputs": [],
      "source": [
        "# Show final Q-table\n",
        "print(\"Learned Q-Table:\")\n",
        "print(q_table.round(2))\n",
        "\n",
        "# Trace optimal route\n",
        "state = 0\n",
        "path = [location_names[state]]\n",
        "while state != 4:\n",
        "    next_states = graph[state]\n",
        "    next_state = max(next_states, key=lambda a: q_table[state][a])\n",
        "    path.append(location_names[next_state])\n",
        "    state = next_state\n",
        "\n",
        "print(\"\\nOptimal route from marsh_harbor:\")\n",
        "print(\" â†’ \".join(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 872
        },
        "id": "3WZY4vbV7rGT",
        "outputId": "8c5a7055-4ea0-4586-d49b-1971908158a3"
      },
      "outputs": [],
      "source": [
        "# Visualize the graph with hurricane levels\n",
        "G = nx.DiGraph()\n",
        "for node, neighbors in graph.items():\n",
        "    for neighbor in neighbors:\n",
        "        G.add_edge(location_names[node], location_names[neighbor])\n",
        "\n",
        "pos = nx.spring_layout(G, seed=42)\n",
        "node_colors = [hurricane_levels[i] for i in graph]\n",
        "\n",
        "# Create color normalization based on hurricane level range\n",
        "norm = mcolors.Normalize(vmin=min(hurricane_levels.values()), vmax=max(hurricane_levels.values()))\n",
        "sm = plt.cm.ScalarMappable(cmap=plt.cm.coolwarm, norm=norm)\n",
        "sm.set_array([])  # Needed to avoid warning\n",
        "\n",
        "# Draw graph\n",
        "nx.draw(G, pos, with_labels=True, node_color=[hurricane_levels[n] for n in graph],\n",
        "        cmap=plt.cm.coolwarm, node_size=1500, font_size=10)\n",
        "\n",
        "nx.draw_networkx_edge_labels(\n",
        "    G,\n",
        "    pos,\n",
        "    edge_labels={(location_names[i], location_names[j]): f\"to {location_names[j]}\" for i in graph for j in graph[i]}\n",
        ")\n",
        "\n",
        "plt.title(\"Evacuation Graph with Hurricane Severity\")\n",
        "\n",
        "# Attach colorbar to the current plot\n",
        "plt.colorbar(sm, label=\"Hurricane Level\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJUVj2Vf9_fU"
      },
      "source": [
        "## Discussion\n",
        "\n",
        "This prototype demonstrates the use of Q-learning to model adaptive agent behavior in response to environmental risk factors like hurricanes. The model successfully learns to avoid high-risk locations and prioritize safer routes based on the reward structure.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
